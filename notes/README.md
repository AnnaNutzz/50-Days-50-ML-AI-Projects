# Notes

This section contains my **learning notes** for core Machine Learning concepts, written alongside building the models **from scratch**.

These notes exist because:
- reading theory alone didn’t stick
- writing code alone wasn’t enough
- explaining things *to myself* made them click

So this folder is where the **thinking lives**, not just the results.

---

## What These Notes Are

- Personal, concept-first explanations  
- Written while learning, not after mastering  
- Focused on *why things work*, not just formulas  
- Meant to be revisited during revision, interviews, or future projects  

They are intentionally slower and more detailed than code comments.

---

## What These Notes Are NOT

- Not polished lecture notes  
- Not copied from textbooks or blogs  
- Not library-specific documentation  
- Not meant to impress — meant to **clarify**

If something is explained in a slightly long way, it’s because it didn’t click immediately.

---

## Topics Covered (So Far)

- **Linear Regression**
  - best-fit line intuition
  - loss functions
  - gradient descent behavior

- **Logistic Regression**
  - sigmoid function
  - odds & log-odds
  - probability vs decision
  - decision boundary

More topics will be added as the learning progresses.

---

## How to Read This Section

- Start with the topic file (e.g. `Linear_Regression.md`)
- Use it alongside the corresponding project code
- Treat it as a mental model refresher, not a cheat sheet

These notes are written for **future-me** first — others are welcome to read along.

---

Part of the **50 Days learning challenge**.  
Code teaches *how*.  
Notes teach *why*.
